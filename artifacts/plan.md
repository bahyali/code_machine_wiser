# Project Plan: LLM-Powered Q&A System for PostgreSQL

**Version:** 1.0
**Date:** 2023-10-28
**Generated By:** AI Software Architect Model v2.1

## 1. Project Overview

*   **Goal:** To develop an intelligent system that enables users to interact with a PostgreSQL database using natural language queries, leveraging GPT-4o for intent analysis, SQL generation, and response formulation.
*   **High-Level Requirements Summary:**
    *   Accept natural language user queries.
    *   Analyze user query intent (chit-chat, data retrieval, insights) using LLM (GPT-4o).
    *   Handle chit-chat intent by generating conversational responses.
    *   Handle data retrieval intent by generating/executing SQL and returning data.
    *   Handle insight generation intent by accessing DB schema, generating/executing (potentially iterative) SQL queries.
    *   Access and interpret PostgreSQL database schema.
    *   Execute SQL queries and handle errors, including attempts at correction.
    *   Compile retrieved data and generate natural language responses using LLM.
    *   Format data in responses professionally (whole numbers for counts, SAR for revenue).
*   **Key Assumptions:**
    *   A PostgreSQL database instance is available and network-accessible.
    *   The schema of the target PostgreSQL database is reasonably well-defined.
    *   Reliable access to the GPT-4o API is available (API key, endpoint).
    *   Primary interaction is text-based via an API.
    *   User authentication and GUI are out of scope for MVP.
    *   The system will primarily perform read-only operations on the user's database.

## 2. Core Architecture

*   **Architectural Style:** Modular Monolith with Service-Oriented Components (as per provided "System Architecture Blueprint").
*   **Technology Stack:**
    *   Frontend: None (Text-based API as per requirements).
    *   Backend: Python 3.9+ / FastAPI.
    *   Database:
        *   User Data: PostgreSQL (external, provided by user).
        *   Application Configuration: Environment variables and/or configuration files (e.g., `config.yaml`). No separate application DB for MVP.
    *   Messaging/Queues: None for MVP. Synchronous operations initially.
    *   Deployment: Docker containers, target cloud-agnostic (e.g., AWS, Azure, GCP).
    *   Other Key Libraries/Tools: `openai` (OpenAI Python client), `psycopg2-binary` (PostgreSQL driver), `Pydantic` (data validation), `python-dotenv` (environment variables), `PyYAML` (config files), `logging` (standard Python logging). Consider `Langchain` for complex LLM workflows if beneficial.
*   **Key Components/Services:** (Based on Architecture Document Section 3.5)
    *   **API Handler:** FastAPI routers for request/response handling and input validation.
    *   **Query Orchestrator:** Manages the overall query processing flow.
    *   **Intent Analysis Module:** Determines user intent using LLM.
    *   **Chit-Chat Handler:** Generates conversational responses for chit-chat intent.
    *   **DB Schema Manager:** Fetches, caches (optional), and provides database schema.
    *   **SQL Generation Module:** Generates SQL queries using LLM and schema info.
    *   **SQL Execution Module:** Executes SQL against user's PostgreSQL DB.
    *   **SQL Error & Correction Module:** Analyzes SQL errors and attempts correction using LLM.
    *   **Response Synthesis Module:** Generates natural language responses from data using LLM.
    *   **Data Formatter & Validator:** Formats data (counts, SAR) and validates presentation.
    *   **LLM Interaction Service:** Wrapper for OpenAI API communication, managing prompts.
    *   **Configuration Service:** Manages application settings (API keys, LLM model, etc.).
    *   *Key diagrams planned: C4 Component Diagram for Application Service (see I1.T3), Sequence Diagram for Insight Generation (see I4.T1).*
*   **Data Model Overview:**
    *   The system primarily interacts with an *external* user-provided PostgreSQL database whose schema is dynamic.
    *   Internal data model for MVP is minimal, focusing on configuration managed via files/env vars.
    *   *Key diagrams planned: No internal ERD for MVP. Schema of external DB is discovered dynamically.*
*   **API Contract Style:** RESTful (OpenAPI v3).
    *   *Planned specification file generation: Initial OpenAPI spec (see I1.T4), refined in subsequent iterations.*
*   **Communication Patterns:**
    *   User to System: Synchronous HTTPS requests to FastAPI backend.
    *   Internal Components: Python method calls.
    *   System to LLM Service: Synchronous HTTPS API calls to OpenAI.
    *   System to User PostgreSQL DB: Synchronous SQL execution.
    *   *Mention relevant sequence diagrams if applicable: Sequence Diagram for Insight Generation (see I4.T1).*

## 2.1. Key Architectural Artifacts Planned

*   **C4 Level 1 System Context Diagram (PlantUML):** To visualize the system's position within its operational environment and interactions with users and external systems. (Created in I1.T2)
*   **C4 Level 2 Container Diagram (PlantUML):** To show the main deployable units (containers) of the system and their interactions. (Created in I1.T2)
*   **C4 Level 3 Component Diagram for Application Service (PlantUML):** To detail the internal components of the Application Service and their relationships, guiding development. (Created in I1.T3)
*   **API Specification (OpenAPI v3 YAML):** To define the RESTful API endpoints, request/response schemas for user interaction. (Initial version created in I1.T4, refined iteratively)
*   **Sequence Diagram: Insight Generation Flow (PlantUML):** To illustrate the complex interaction sequence for handling "asking for insights" intent, including iterative querying and error handling. (Created in I4.T1)
*   **Sequence Diagram: Data Retrieval Flow (PlantUML):** To illustrate the interaction sequence for handling "data retrieval" intent. (Created in I3.T1)
*   **Configuration File Schema (Markdown/JSON Schema):** To define the structure and allowed values for system configuration files (e.g., `config.yaml`). (Created in I1.T5)

## 3. Directory Structure

*   **Root Directory:** `llm_sql_query_system/`
*   **Structure Definition:**
    ```
    llm_sql_query_system/
    ├── src/                      # Source code
    │   ├── main.py               # FastAPI application entry point
    │   ├── api/                  # API endpoint handlers/routers
    │   │   └── v1/
    │   │       └── query_router.py
    │   ├── core/                 # Core application logic and services
    │   │   ├── config.py         # Configuration loading and management
    │   │   ├── orchestrator.py
    │   │   ├── llm_interaction_service.py
    │   │   └── ... (other component modules: intent, sql_gen, etc.)
    │   ├── models/               # Pydantic models for API requests/responses, data structures
    │   │   └── R__init__.py
    │   │   └── query_models.py
    │   ├── services/             # Business logic services (components from Arch Doc 3.5)
    │   │   ├── intent_analyzer.py
    │   │   ├── chitchat_handler.py
    │   │   ├── schema_manager.py
    │   │   ├── sql_generator.py
    │   │   ├── sql_executor.py
    │   │   ├── error_corrector.py
    │   │   ├── response_synthesizer.py
    │   │   └── formatter_validator.py
    │   ├── prompts/              # LLM prompt templates (e.g., .txt or .yaml files)
    │   │   ├── intent_analysis.txt
    │   │   └── sql_generation.txt
    │   └── utils/                # Utility functions
    ├── tests/                    # Unit and integration tests
    │   ├── unit/
    │   └── integration/
    ├── docs/                     # Documentation and design artifacts
    │   ├── diagrams/             # PlantUML source files (.puml), Mermaid source files (.mmd)
    │   │   ├── c4_level1_context.puml
    │   │   ├── c4_level2_container.puml
    │   │   ├── c4_level3_component_app_service.puml
    │   │   ├── sequence_data_retrieval.puml
    │   │   └── sequence_insight_generation.puml
    │   └── adr/                  # Architectural Decision Records (optional, Markdown)
    ├── api_docs/                 # Generated API specifications (e.g., OpenAPI YAML)
    │   └── openapi.yaml
    ├── .env.example              # Example environment variables file
    ├── config.example.yaml       # Example configuration file
    ├── Dockerfile                # Docker build instructions
    ├── requirements.txt          # Python dependencies
    ├── README.md                 # Project overview and setup instructions
    └── .gitignore
    ```

## 4. Iteration Plan

*   **Total Iterations Planned:** 5
*   **Iteration Dependencies:** Each iteration generally depends on the completion of the previous one, especially for core functionalities. Specific task dependencies are listed within each task.

---

### Iteration 1: Project Setup, Core API, Basic Orchestration & LLM Interaction Shell

*   **Iteration ID:** `I1`
*   **Goal:** Establish the project foundation, including directory structure, basic FastAPI application, a simple query endpoint, initial C4 diagrams, core configuration management, and a shell for LLM interaction.
*   **Prerequisites:** None
*   **Tasks:**
    *   **Task 1.1:** Project Scaffolding and Environment Setup
        *   **Task ID:** `I1.T1`
        *   **Description:** Create the project directory structure as defined in Section 3. Initialize `git`. Set up virtual environment and install initial dependencies (`fastapi`, `uvicorn`, `pydantic`, `python-dotenv`, `PyYAML`). Create basic `README.md`, `.gitignore`, `Dockerfile` (basic FastAPI), and `requirements.txt`.
        *   **Agent Type Hint:** `SetupAgent`
        *   **Inputs:** Section 3 (Directory Structure).
        *   **Input Files**: N/A
        *   **Target Files:** `llm_sql_query_system/` (entire structure), `llm_sql_query_system/README.md`, `llm_sql_query_system/.gitignore`, `llm_sql_query_system/Dockerfile`, `llm_sql_query_system/requirements.txt`.
        *   **Deliverables:** Project directory structure, basic project files, initial `requirements.txt`.
        *   **Acceptance Criteria:** Directory structure matches Section 3. `fastapi` app can run a basic "hello world" endpoint. `Dockerfile` can build a runnable image.
        *   **Dependencies:** None
        *   **Parallelizable:** No
    *   **Task 1.2:** Generate C4 Diagrams (Level 1 & 2)
        *   **Task ID:** `I1.T2`
        *   **Description:** Generate the C4 Level 1 System Context Diagram and C4 Level 2 Container Diagram based on the "System Architecture Blueprint" (Sections 3.3, 3.4). Save as PlantUML source files.
        *   **Agent Type Hint:** `DiagrammingAgent`
        *   **Inputs:** "System Architecture Blueprint" document (Sections 3.3, 3.4).
        *   **Input Files**: N/A (information from architecture document)
        *   **Target Files:** `docs/diagrams/c4_level1_context.puml`, `docs/diagrams/c4_level2_container.puml`.
        *   **Deliverables:** PlantUML files for C4 L1 and L2 diagrams.
        *   **Acceptance Criteria:** PlantUML files are syntactically correct and render diagrams accurately reflecting the architecture document.
        *   **Dependencies:** `I1.T1` (for directory structure)
        *   **Parallelizable:** Yes (with I1.T3, I1.T4, I1.T5 if I1.T1 is done)
    *   **Task 1.3:** Generate C4 Component Diagram (Application Service)
        *   **Task ID:** `I1.T3`
        *   **Description:** Generate the C4 Level 3 Component Diagram for the "Application Service" based on the "System Architecture Blueprint" (Section 3.5). Save as a PlantUML source file.
        *   **Agent Type Hint:** `DiagrammingAgent`
        *   **Inputs:** "System Architecture Blueprint" document (Section 3.5).
        *   **Input Files**: N/A (information from architecture document)
        *   **Target Files:** `docs/diagrams/c4_level3_component_app_service.puml`.
        *   **Deliverables:** PlantUML file for C4 L3 Application Service Component diagram.
        *   **Acceptance Criteria:** PlantUML file is syntactically correct and renders a diagram accurately reflecting the components described in the architecture document.
        *   **Dependencies:** `I1.T1`
        *   **Parallelizable:** Yes (with I1.T2, I1.T4, I1.T5 if I1.T1 is done)
    *   **Task 1.4:** Define Initial API Specification (OpenAPI)
        *   **Task ID:** `I1.T4`
        *   **Description:** Create an initial OpenAPI v3 specification (YAML) for the primary query endpoint (e.g., `POST /api/v1/query`). Define basic request (natural language query) and response (text response) schemas using Pydantic models.
        *   **Agent Type Hint:** `APIDefinitionAgent`
        *   **Inputs:** Section 2 (API Contract Style), FR-INPUT-001.
        *   **Input Files**: N/A
        *   **Target Files:** `api_docs/openapi.yaml`, `src/models/query_models.py`.
        *   **Deliverables:** `openapi.yaml` file, Pydantic models for request/response.
        *   **Acceptance Criteria:** `openapi.yaml` is valid OpenAPI v3. Pydantic models are defined. FastAPI can generate docs from this.
        *   **Dependencies:** `I1.T1`
        *   **Parallelizable:** Yes (with I1.T2, I1.T3, I1.T5 if I1.T1 is done)
    *   **Task 1.5:** Implement Configuration Management
        *   **Task ID:** `I1.T5`
        *   **Description:** Implement configuration loading (e.g., for LLM API key, model name) from environment variables and optionally a YAML file (`config.yaml`). Create `src/core/config.py`, `.env.example`, and `config.example.yaml`. Define a schema/structure for `config.yaml` in Markdown.
        *   **Agent Type Hint:** `BackendAgent`
        *   **Inputs:** NFR-MAINT-001, Section 2 (Technology Stack - `python-dotenv`, `PyYAML`).
        *   **Input Files**: N/A
        *   **Target Files:** `src/core/config.py`, `.env.example`, `config.example.yaml`, `docs/config_schema.md`.
        *   **Deliverables:** Configuration loading module, example config files, config schema documentation.
        *   **Acceptance Criteria:** Application can load configuration from .env and/or config.yaml. Sensitive keys (like API key) are loaded from .env.
        *   **Dependencies:** `I1.T1`
        *   **Parallelizable:** Yes (with I1.T2, I1.T3, I1.T4 if I1.T1 is done)
    *   **Task 1.6:** Implement Basic API Handler and Query Orchestrator Shell
        *   **Task ID:** `I1.T6`
        *   **Description:** Implement the FastAPI entry point (`src/main.py`) and the API handler (`src/api/v1/query_router.py`) for the `/api/v1/query` endpoint based on `I1.T4`. Create a shell for the `QueryOrchestrator` (`src/core/orchestrator.py`) that the API handler calls. The orchestrator initially returns a hardcoded response.
        *   **Agent Type Hint:** `BackendAgent`
        *   **Inputs:** `I1.T4` (OpenAPI spec and Pydantic models), Section 2 (Key Components).
        *   **Input Files**: `api_docs/openapi.yaml`, `src/models/query_models.py`
        *   **Target Files:** `src/main.py`, `src/api/v1/query_router.py`, `src/core/orchestrator.py`.
        *   **Deliverables:** Functional API endpoint returning a hardcoded response. Shell for Query Orchestrator.
        *   **Acceptance Criteria:** `POST /api/v1/query` endpoint accepts a query string and returns a predefined JSON response. Orchestrator shell is callable.
        *   **Dependencies:** `I1.T1`, `I1.T4`, `I1.T5`
        *   **Parallelizable:** No
    *   **Task 1.7:** Implement LLM Interaction Service Shell
        *   **Task ID:** `I1.T7`
        *   **Description:** Create a shell for the `LLMInteractionService` (`src/core/llm_interaction_service.py`). It should initialize with configuration (API key, model) from `I1.T5` and have a placeholder method for making LLM calls (e.g., `get_completion(prompt)`). This method can initially return a hardcoded string or log the prompt.
        *   **Agent Type Hint:** `BackendAgent`
        *   **Inputs:** `I1.T5` (Config module), Section 2 (Key Components).
        *   **Input Files**: `src/core/config.py`
        *   **Target Files:** `src/core/llm_interaction_service.py`.
        *   **Deliverables:** Shell `LLMInteractionService` class.
        *   **Acceptance Criteria:** Service can be instantiated. `get_completion` method exists and can be called.
        *   **Dependencies:** `I1.T5`
        *   **Parallelizable:** Yes (can be done alongside I1.T6 if I1.T5 is complete)

---

### Iteration 2: Intent Analysis, Chit-Chat Handling, DB Schema Management

*   **Iteration ID:** `I2`
*   **Goal:** Implement intent analysis, chit-chat response generation, and the ability to fetch and manage database schema information.
*   **Prerequisites:** `I1` (specifically `I1.T5`, `I1.T6`, `I1.T7`)
*   **Tasks:**
    *   **Task 2.1:** Implement LLM Interaction Service (Actual Calls)
        *   **Task ID:** `I2.T1`
        *   **Description:** Enhance `LLMInteractionService` to make actual API calls to GPT-4o using the `openai` library. Implement error handling for API calls (retries, exceptions). The `get_completion` method should take a prompt and return the LLM's response.
        *   **Agent Type Hint:** `BackendAgent`
        *   **Inputs:** `I1.T7` (LLM Service Shell), `I1.T5` (Config), OpenAI API documentation.
        *   **Input Files**: `src/core/llm_interaction_service.py`, `src/core/config.py`
        *   **Target Files:** `src/core/llm_interaction_service.py` (updated).
        *   **Deliverables:** Functional `LLMInteractionService` capable of communicating with GPT-4o.
        *   **Acceptance Criteria:** Service can successfully send a prompt to GPT-4o and receive a response. API errors are handled gracefully.
        *   **Dependencies:** `I1.T7`, `I1.T5`
        *   **Parallelizable:** No
    *   **Task 2.2:** Implement Intent Analysis Module
        *   **Task ID:** `I2.T2`
        *   **Description:** Create `IntentAnalysisModule` (`src/services/intent_analyzer.py`). This module will use `LLMInteractionService` to classify user query intent (chit-chat, data retrieval, insights) as per FR-INTENT-001. Develop initial prompt templates for intent analysis and store them in `src/prompts/intent_analysis.txt`.
        *   **Agent Type Hint:** `BackendAgent` / `LLMAgent`
        *   **Inputs:** FR-INTENT-001, `I2.T1` (LLM Interaction Service).
        *   **Input Files**: `src/core/llm_interaction_service.py`, `src/core/config.py`
        *   **Target Files:** `src/services/intent_analyzer.py`, `src/prompts/intent_analysis.txt`.
        *   **Deliverables:** `IntentAnalysisModule` and prompt template(s).
        *   **Acceptance Criteria:** Module can take a user query and return a classified intent (e.g., "CHITCHAT", "DATA_RETRIEVAL", "INSIGHTS"). NFR-ACC-001 target: 95% accuracy on a predefined test set of queries.
        *   **Dependencies:** `I2.T1`
        *   **Parallelizable:** Yes
    *   **Task 2.3:** Implement Chit-Chat Handler Module
        *   **Task ID:** `I2.T3`
        *   **Description:** Create `ChitChatHandlerModule` (`src/services/chitchat_handler.py`). This module will use `LLMInteractionService` to generate conversational responses for "chit-chat" intents (FR-INTENT-001.1). Develop initial prompt templates for chit-chat responses.
        *   **Agent Type Hint:** `BackendAgent` / `LLMAgent`
        *   **Inputs:** FR-INTENT-001.1, `I2.T1` (LLM Interaction Service).
        *   **Input Files**: `src/core/llm_interaction_service.py`, `src/core/config.py`
        *   **Target Files:** `src/services/chitchat_handler.py`, `src/prompts/chitchat_response.txt`.
        *   **Deliverables:** `ChitChatHandlerModule` and prompt template(s).
        *   **Acceptance Criteria:** Module can take a user query (classified as chit-chat) and generate a relevant conversational response. NFR-PERF-001 target: response within 10s.
        *   **Dependencies:** `I2.T1`
        *   **Parallelizable:** Yes
    *   **Task 2.4:** Implement DB Schema Manager
        *   **Task ID:** `I2.T4`
        *   **Description:** Create `DBSchemaManager` (`src/services/schema_manager.py`). This module will connect to the user's PostgreSQL database (connection details from config) and fetch schema information (tables, columns, types, relationships) as per FR-DB-001. Implement methods to get schema as a structured string/JSON suitable for LLM prompts. Add `psycopg2-binary` to `requirements.txt`.
        *   **Agent Type Hint:** `DatabaseAgent` / `BackendAgent`
        *   **Inputs:** FR-DB-001, `I1.T5` (Config). PostgreSQL documentation on information_schema.
        *   **Input Files**: `src/core/config.py`
        *   **Target Files:** `src/services/schema_manager.py`, `requirements.txt` (updated).
        *   **Deliverables:** `DBSchemaManager` capable of fetching and representing PostgreSQL schema.
        *   **Acceptance Criteria:** Module can connect to a sample PostgreSQL DB and retrieve its schema. Schema representation is clear and comprehensive for LLM consumption.
        *   **Dependencies:** `I1.T5`
        *   **Parallelizable:** Yes
    *   **Task 2.5:** Integrate Intent Analysis & Chit-Chat into Orchestrator
        *   **Task ID:** `I2.T5`
        *   **Description:** Update `QueryOrchestrator` (`src/core/orchestrator.py`) to use `IntentAnalysisModule`. If intent is "chit-chat", use `ChitChatHandlerModule` to generate and return the response. For other intents, return a placeholder message for now.
        *   **Agent Type Hint:** `BackendAgent`
        *   **Inputs:** `I1.T6` (Orchestrator Shell), `I2.T2` (Intent Module), `I2.T3` (Chit-Chat Module).
        *   **Input Files**: `src/core/orchestrator.py`, `src/services/intent_analyzer.py`, `src/services/chitchat_handler.py`
        *   **Target Files:** `src/core/orchestrator.py` (updated).
        *   **Deliverables:** Orchestrator that handles chit-chat intent end-to-end.
        *   **Acceptance Criteria:** API can now receive a query, classify it, and if chit-chat, respond using LLM. Other intents are acknowledged.
        *   **Dependencies:** `I1.T6`, `I2.T2`, `I2.T3`
        *   **Parallelizable:** No

---

### Iteration 3: Data Retrieval, Basic Response Synthesis & Formatting

*   **Iteration ID:** `I3`
*   **Goal:** Implement the "data retrieval" intent flow, including SQL generation, execution, basic response synthesis from data, and data formatting.
*   **Prerequisites:** `I2` (specifically `I2.T1`, `I2.T4`, `I2.T5`)
*   **Tasks:**
    *   **Task 3.1:** Generate Sequence Diagram for Data Retrieval Flow
        *   **Task ID:** `I3.T1`
        *   **Description:** Create a PlantUML sequence diagram illustrating the typical flow for a "data retrieval" intent, from API request through intent analysis, SQL generation, SQL execution, data compilation, response synthesis, and API response.
        *   **Agent Type Hint:** `DiagrammingAgent`
        *   **Inputs:** FR-INTENT-001.2, FR-SQL-001, FR-SQL-003, FR-RESP-001, FR-RESP-002, System Architecture.
        *   **Input Files**: N/A (based on requirements and planned component interactions)
        *   **Target Files:** `docs/diagrams/sequence_data_retrieval.puml`.
        *   **Deliverables:** PlantUML file for the data retrieval sequence diagram.
        *   **Acceptance Criteria:** PlantUML file is syntactically correct and accurately depicts the data retrieval flow involving relevant components.
        *   **Dependencies:** `I1.T1`
        *   **Parallelizable:** Yes
    *   **Task 3.2:** Implement SQL Generation Module (Data Retrieval)
        *   **Task ID:** `I3.T2`
        *   **Description:** Create `SQLGenerationModule` (`src/services/sql_generator.py`). This module will use `LLMInteractionService` and schema information from `DBSchemaManager` to translate a user's natural language query (intent: data retrieval) into a SQL query (FR-SQL-001). Develop initial prompt templates for SQL generation.
        *   **Agent Type Hint:** `BackendAgent` / `LLMAgent`
        *   **Inputs:** FR-SQL-001, `I2.T1` (LLM Service), `I2.T4` (Schema Manager).
        *   **Input Files**: `src/core/llm_interaction_service.py`, `src/services/schema_manager.py`, `src/core/config.py`
        *   **Target Files:** `src/services/sql_generator.py`, `src/prompts/sql_generation_retrieval.txt`.
        *   **Deliverables:** `SQLGenerationModule` and SQL generation prompt template(s).
        *   **Acceptance Criteria:** Module can take a user query and schema info, and generate a syntactically plausible SQL query. NFR-ACC-002 target: 95% correctness for translatable queries.
        *   **Dependencies:** `I2.T1`, `I2.T4`
        *   **Parallelizable:** Yes
    *   **Task 3.3:** Implement SQL Execution Module
        *   **Task ID:** `I3.T3`
        *   **Description:** Create `SQLExecutionModule` (`src/services/sql_executor.py`). This module will take a generated SQL query, connect to the user's PostgreSQL DB (using `DBSchemaManager`'s connection or similar config), execute the query (FR-SQL-003), and return the results. Handle basic execution errors (e.g., connection issues, syntax errors reported by DB).
        *   **Agent Type Hint:** `DatabaseAgent` / `BackendAgent`
        *   **Inputs:** FR-SQL-003, `I2.T4` (for DB connection config).
        *   **Input Files**: `src/core/config.py` (for DB connection details)
        *   **Target Files:** `src/services/sql_executor.py`.
        *   **Deliverables:** `SQLExecutionModule` capable of executing SQL and returning results or errors.
        *   **Acceptance Criteria:** Module can execute valid SQL SELECT queries against a sample PostgreSQL DB and return fetched data. Basic DB errors are caught and reported.
        *   **Dependencies:** `I1.T5` (for DB connection details, potentially enhance config)
        *   **Parallelizable:** Yes
    *   **Task 3.4:** Implement Response Synthesis Module (Basic)
        *   **Task ID:** `I3.T4`
        *   **Description:** Create `ResponseSynthesisModule` (`src/services/response_synthesizer.py`). For data retrieval, this module will take the original user query and the data fetched by `SQLExecutionModule`, then use `LLMInteractionService` to generate a natural language response summarizing or presenting the data (FR-RESP-001, FR-RESP-002).
        *   **Agent Type Hint:** `BackendAgent` / `LLMAgent`
        *   **Inputs:** FR-RESP-001, FR-RESP-002, `I2.T1` (LLM Service).
        *   **Input Files**: `src/core/llm_interaction_service.py`, `src/core/config.py`
        *   **Target Files:** `src/services/response_synthesizer.py`, `src/prompts/response_synthesis_retrieval.txt`.
        *   **Deliverables:** `ResponseSynthesisModule` and response synthesis prompt template(s).
        *   **Acceptance Criteria:** Module can take query results and generate a coherent natural language response.
        *   **Dependencies:** `I2.T1`
        *   **Parallelizable:** Yes
    *   **Task 3.5:** Implement Data Formatter & Validator Module
        *   **Task ID:** `I3.T5`
        *   **Description:** Create `DataFormatterValidatorModule` (`src/services/formatter_validator.py`). This module will implement logic to format data as per FR-PRES-001 and FR-VALID-001 (whole numbers for counts, SAR currency format for revenues). This module will be used by `ResponseSynthesisModule` or the Orchestrator before sending the final response.
        *   **Agent Type Hint:** `BackendAgent`
        *   **Inputs:** FR-PRES-001, FR-VALID-001.
        *   **Input Files**: N/A
        *   **Target Files:** `src/services/formatter_validator.py`.
        *   **Deliverables:** `DataFormatterValidatorModule`.
        *   **Acceptance Criteria:** Module correctly formats numbers as whole counts and monetary values in SAR format (e.g., "1,234.50 SAR"). Validation ensures correct application.
        *   **Dependencies:** None
        *   **Parallelizable:** Yes
    *   **Task 3.6:** Integrate Data Retrieval Flow into Orchestrator
        *   **Task ID:** `I3.T6`
        *   **Description:** Update `QueryOrchestrator` to handle "data retrieval" intent. It will:
            1.  Get schema from `DBSchemaManager`.
            2.  Generate SQL using `SQLGenerationModule`.
            3.  Execute SQL using `SQLExecutionModule`.
            4.  (If successful) Synthesize response using `ResponseSynthesisModule`.
            5.  (If successful) Format data using `DataFormatterValidatorModule`.
            6.  Return the final response. Basic error handling (e.g., if SQL execution fails, return an error message).
        *   **Agent Type Hint:** `BackendAgent`
        *   **Inputs:** `I2.T5` (Orchestrator), `I3.T2`, `I3.T3`, `I3.T4`, `I3.T5`, `I2.T4`.
        *   **Input Files**: `src/core/orchestrator.py`, and all module files from dependencies.
        *   **Target Files:** `src/core/orchestrator.py` (updated).
        *   **Deliverables:** Orchestrator capable of handling data retrieval intent end-to-end.
        *   **Acceptance Criteria:** API can now process data retrieval queries: generate SQL, execute, get data, formulate, format, and return a response. NFR-PERF-002 target: response within 30s for simple queries.
        *   **Dependencies:** `I2.T5`, `I3.2`, `I3.3`, `I3.4`, `I3.5`, `I2.T4`
        *   **Parallelizable:** No

---

### Iteration 4: Insight Generation & SQL Error Handling

*   **Iteration ID:** `I4`
*   **Goal:** Implement "asking for insights" intent, including iterative querying and advanced SQL error handling with correction attempts.
*   **Prerequisites:** `I3` (specifically `I3.6`)
*   **Tasks:**
    *   **Task 4.1:** Generate Sequence Diagram for Insight Generation Flow
        *   **Task ID:** `I4.T1`
        *   **Description:** Create/Refine the PlantUML sequence diagram for "Insight Generation Intent Handling" based on "System Architecture Blueprint" (Section 3.7), detailing iterative querying, schema access, SQL generation/execution, error correction, and response synthesis.
        *   **Agent Type Hint:** `DiagrammingAgent`
        *   **Inputs:** "System Architecture Blueprint" (Section 3.7), FR-INTENT-001.3, FR-SQL-002, FR-SQL-004, FR-ERROR-001.
        *   **Input Files**: N/A (information from architecture document and requirements)
        *   **Target Files:** `docs/diagrams/sequence_insight_generation.puml`.
        *   **Deliverables:** PlantUML file for the insight generation sequence diagram.
        *   **Acceptance Criteria:** PlantUML file is syntactically correct and accurately depicts the complex insight generation flow.
        *   **Dependencies:** `I1.T1`
        *   **Parallelizable:** Yes
    *   **Task 4.2:** Enhance SQL Generation Module (Insights)
        *   **Task ID:** `I4.T2`
        *   **Description:** Extend `SQLGenerationModule` to support "insight generation" (FR-SQL-002). This may involve different prompting strategies, potentially generating multiple queries or more complex queries based on schema and insight type. Add prompt templates for insight SQL generation.
        *   **Agent Type Hint:** `BackendAgent` / `LLMAgent`
        *   **Inputs:** FR-SQL-002, `I3.T2` (SQL Gen Module).
        *   **Input Files**: `src/services/sql_generator.py`, `src/core/llm_interaction_service.py`, `src/services/schema_manager.py`
        *   **Target Files:** `src/services/sql_generator.py` (updated), `src/prompts/sql_generation_insight.txt`.
        *   **Deliverables:** Enhanced `SQLGenerationModule`.
        *   **Acceptance Criteria:** Module can generate SQL queries suitable for gathering data for insights, potentially based on different prompts than simple retrieval.
        *   **Dependencies:** `I3.T2`
        *   **Parallelizable:** Yes
    *   **Task 4.3:** Implement SQL Error & Correction Module
        *   **Task ID:** `I4.T3`
        *   **Description:** Create `SQLErrorCorrectionModule` (`src/services/error_corrector.py`). This module will take a failed SQL query and its error message, use `LLMInteractionService` to analyze and attempt correction (FR-ERROR-001). It should manage retry attempts for corrected queries.
        *   **Agent Type Hint:** `BackendAgent` / `LLMAgent`
        *   **Inputs:** FR-ERROR-001, `I2.T1` (LLM Service).
        *   **Input Files**: `src/core/llm_interaction_service.py`, `src/core/config.py`
        *   **Target Files:** `src/services/error_corrector.py`, `src/prompts/sql_error_correction.txt`.
        *   **Deliverables:** `SQLErrorCorrectionModule` and error correction prompt template(s).
        *   **Acceptance Criteria:** Module can analyze a SQL error, suggest a correction using LLM, and manage retries. NFR-REL-001 target: 90% success for common errors.
        *   **Dependencies:** `I2.T1`
        *   **Parallelizable:** Yes
    *   **Task 4.4:** Enhance Response Synthesis Module (Insights)
        *   **Task ID:** `I4.T4`
        *   **Description:** Extend `ResponseSynthesisModule` to handle data compiled from (potentially multiple) queries for insights. This may involve more complex summarization or explanation by the LLM. Add prompt templates for insight response synthesis.
        *   **Agent Type Hint:** `BackendAgent` / `LLMAgent`
        *   **Inputs:** `I3.T4` (Response Synth Module).
        *   **Input Files**: `src/services/response_synthesizer.py`, `src/core/llm_interaction_service.py`
        *   **Target Files:** `src/services/response_synthesizer.py` (updated), `src/prompts/response_synthesis_insight.txt`.
        *   **Deliverables:** Enhanced `ResponseSynthesisModule`.
        *   **Acceptance Criteria:** Module can synthesize a comprehensive insight from potentially multiple datasets. NFR-ACC-003 target: 75% relevance.
        *   **Dependencies:** `I3.T4`
        *   **Parallelizable:** Yes
    *   **Task 4.5:** Integrate Insight Generation & Error Handling into Orchestrator
        *   **Task ID:** `I4.T5`
        *   **Description:** Update `QueryOrchestrator` to handle "asking for insights" intent. This involves:
            1.  Using `SQLGenerationModule` (for insights).
            2.  Looping for iterative querying if needed (FR-SQL-004), potentially using LLM to decide if more data is needed.
            3.  Using `SQLExecutionModule` and integrating `SQLErrorCorrectionModule` if execution fails.
            4.  Compiling all data and using `ResponseSynthesisModule` (for insights).
            5.  Using `DataFormatterValidatorModule`.
        *   **Agent Type Hint:** `BackendAgent`
        *   **Inputs:** `I3.6` (Orchestrator), `I4.T2`, `I4.T3`, `I4.T4`.
        *   **Input Files**: `src/core/orchestrator.py`, and all module files from dependencies.
        *   **Target Files:** `src/core/orchestrator.py` (updated).
        *   **Deliverables:** Orchestrator capable of handling insight generation, including iterative querying and error correction.
        *   **Acceptance Criteria:** API can process insight queries, potentially making multiple DB calls, correcting SQL errors, and generating a synthesized insight. NFR-PERF-003 target: 90s for typical insights.
        *   **Dependencies:** `I3.6`, `I4.T2`, `I4.T3`, `I4.T4`
        *   **Parallelizable:** No

---

### Iteration 5: Testing, Logging, Monitoring & Finalization

*   **Iteration ID:** `I5`
*   **Goal:** Implement comprehensive testing, robust logging, basic monitoring hooks, finalize documentation, and prepare for initial deployment.
*   **Prerequisites:** `I4`
*   **Tasks:**
    *   **Task 5.1:** Implement Unit Tests for All Modules
        *   **Task ID:** `I5.T1`
        *   **Description:** Write unit tests for all core modules and services (Intent Analysis, SQL Generation, Execution, Error Correction, Response Synthesis, Formatting, Config, LLM Interaction, Schema Manager). Aim for high code coverage. Use `pytest`.
        *   **Agent Type Hint:** `TestingAgent`
        *   **Inputs:** All source code modules from `src/`.
        *   **Input Files**: `src/` directory.
        *   **Target Files:** `tests/unit/` (populated with test files).
        *   **Deliverables:** Suite of unit tests.
        *   **Acceptance Criteria:** Unit tests pass. Code coverage > 80%.
        *   **Dependencies:** `I4.T5` (all modules implemented)
        *   **Parallelizable:** Yes (tests for different modules can be written in parallel)
    *   **Task 5.2:** Implement Integration Tests for Core Flows
        *   **Task ID:** `I5.T2`
        *   **Description:** Write integration tests for the main flows: chit-chat, data retrieval, and insight generation. These tests will involve multiple components interacting. Mock external services (LLM API, PostgreSQL DB) where appropriate.
        *   **Agent Type Hint:** `TestingAgent`
        *   **Inputs:** `src/core/orchestrator.py`, `src/api/v1/query_router.py`.
        *   **Input Files**: `src/` directory.
        *   **Target Files:** `tests/integration/` (populated with test files).
        *   **Deliverables:** Suite of integration tests.
        *   **Acceptance Criteria:** Integration tests pass, verifying component collaboration for key scenarios.
        *   **Dependencies:** `I4.T5`
        *   **Parallelizable:** Yes
    *   **Task 5.3:** Implement Comprehensive Logging
        *   **Task ID:** `I5.T3`
        *   **Description:** Integrate structured logging (e.g., JSON format) throughout the application. Log key events: incoming requests, intent classification, generated SQL (sanitized), SQL execution status, errors, LLM prompts/responses (or IDs/metadata for brevity/cost), final responses. Configure log levels.
        *   **Agent Type Hint:** `BackendAgent`
        *   **Inputs:** All source code modules. Architecture doc section 3.8 (Logging & Monitoring).
        *   **Input Files**: `src/` directory.
        *   **Target Files:** All `*.py` files in `src/` updated with logging statements. `src/core/config.py` may need logging setup.
        *   **Deliverables:** Application with comprehensive logging.
        *   **Acceptance Criteria:** Logs are generated in a structured format. Key events and errors are logged appropriately. Log levels are configurable.
        *   **Dependencies:** `I4.T5`
        *   **Parallelizable:** No (requires modifying many files, but can be done by multiple agents on different modules)
    *   **Task 5.4:** Setup Basic CI/CD Pipeline
        *   **Task ID:** `I5.T4`
        *   **Description:** Create a basic CI/CD pipeline script (e.g., GitHub Actions workflow YAML). The pipeline should: lint code, run unit and integration tests, build Docker image. Optionally, validate OpenAPI spec.
        *   **Agent Type Hint:** `DevOpsAgent`
        *   **Inputs:** `Dockerfile`, test suites, linter configuration.
        *   **Input Files**: `Dockerfile`, `requirements.txt`, `tests/`
        *   **Target Files:** `.github/workflows/ci.yaml` (or equivalent for other CI systems).
        *   **Deliverables:** CI pipeline configuration file.
        *   **Acceptance Criteria:** Pipeline runs automatically on commits/PRs. Linter, tests, and Docker build pass.
        *   **Dependencies:** `I1.T1`, `I5.T1`, `I5.T2`
        *   **Parallelizable:** Yes
    *   **Task 5.5:** Finalize README and API Documentation
        *   **Task ID:** `I5.T5`
        *   **Description:** Update `README.md` with comprehensive setup, configuration, and usage instructions. Ensure `api_docs/openapi.yaml` is up-to-date and well-documented. Regenerate/verify FastAPI auto-docs. Review all diagrams in `docs/diagrams/` for accuracy.
        *   **Agent Type Hint:** `DocumentationAgent`
        *   **Inputs:** Existing `README.md`, `api_docs/openapi.yaml`, `docs/diagrams/`.
        *   **Input Files**: `README.md`, `api_docs/openapi.yaml`, `docs/diagrams/`
        *   **Target Files:** `README.md` (updated), `api_docs/openapi.yaml` (updated/verified).
        *   **Deliverables:** Finalized project documentation.
        *   **Acceptance Criteria:** README is clear and complete. OpenAPI spec is accurate. Diagrams are consistent with the final implementation.
        *   **Dependencies:** `I4.T5` (all features complete)
        *   **Parallelizable:** Yes

## 5. Verification and Integration Strategy

*   **Testing Levels:**
    *   **Unit Tests:** Each module/component will have unit tests (`pytest`) covering its specific logic, with mocks for external dependencies (LLM, DB). (See I5.T1)
    *   **Integration Tests:** Tests will cover the interaction between components for key flows (chit-chat, data retrieval, insight generation), mocking only external systems like the actual OpenAI API and the user's PostgreSQL DB. (See I5.T2)
    *   **E2E Tests (Manual for MVP):** Manual end-to-end testing by sending queries via an API client (e.g., Postman, curl) to a deployed instance connected to a test PostgreSQL DB and a real LLM API endpoint.
*   **CI/CD:**
    *   A basic CI pipeline (e.g., GitHub Actions) will be set up. (See I5.T4)
    *   On every commit/PR to main branches:
        1.  Code Linting (e.g., Flake8, Black).
        2.  Run Unit Tests.
        3.  Run Integration Tests.
        4.  Validate OpenAPI specification (`api_docs/openapi.yaml`) syntax.
        5.  Build Docker image.
    *   (Future) Automated deployment to a staging environment.
*   **Code Quality Gates:**
    *   Linter success (no errors).
    *   All unit and integration tests must pass.
    *   Minimum code coverage target (e.g., 80% for unit tests).
    *   PRs require at least one review (simulated by a "ReviewAgent" or manual check).
*   **Artifact Validation:**
    *   **OpenAPI Specification:** Validated against OpenAPI v3 schema during CI. FastAPI's auto-generated docs serve as a visual check.
    *   **PlantUML/Mermaid Diagrams:** Syntax checked by attempting to render them (can be part of a documentation build step or manual check). Visual review against the implemented system during iteration reviews or by a `DocumentationAgent`.
    *   **Configuration Schema:** Manually reviewed for clarity and completeness.

## 6. Glossary

*   **ADR:** Architectural Decision Record. A document that captures an important architectural decision, its context, and consequences.
*   **API:** Application Programming Interface.
*   **C4 Model:** A model for visualizing software architecture at different levels of detail (Context, Containers, Components, Code).
*   **CI/CD:** Continuous Integration/Continuous Deployment (or Delivery).
*   **DB:** Database.
*   **FastAPI:** A modern, fast web framework for building APIs with Python.
*   **FR:** Functional Requirement.
*   **GPT-4o:** Generative Pre-trained Transformer 4 Omni - a specific Large Language Model by OpenAI.
*   **LLM:** Large Language Model.
*   **MVP:** Minimum Viable Product.
*   **NFR:** Non-Functional Requirement.
*   **OpenAPI:** A standard, language-agnostic specification for RESTful APIs.
*   **PlantUML:** A tool to create UML diagrams from a textual description.
*   **PostgreSQL:** An open-source object-relational database system.
*   **Pydantic:** A Python library for data validation and settings management.
*   **SAR:** Saudi Riyal (currency).
*   **SQL:** Structured Query Language.