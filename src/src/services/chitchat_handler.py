import logging
import os
from typing import Any

# Assuming LLMInteractionService is in src.core
from core.llm_interaction_service import LLMInteractionService

logger = logging.getLogger(__name__)

# Define the path to the prompt templates relative to the project root
# This might need adjustment based on the final deployment structure,
# but for development, assume running from project root or src.
# A more robust approach would load this path from config.
PROMPT_TEMPLATE_PATH = os.path.join(os.path.dirname(__file__), "..", "prompts", "chitchat_response.txt")

class ChitChatHandlerModule:
    """
    Handles user queries classified as general chit-chat.

    Uses the LLMInteractionService to generate conversational responses
    based on predefined prompt templates.
    """

    def __init__(self, llm_service: LLMInteractionService):
        """
        Initializes the ChitChatHandlerModule.

        Args:
            llm_service: An instance of LLMInteractionService.
        """
        self.llm_service: LLMInteractionService = llm_service
        self._prompt_template: str = self._load_prompt_template()
        logger.info("ChitChatHandlerModule initialized.")

    def _load_prompt_template(self) -> str:
        """
        Loads the chit-chat response prompt template from a file.
        """
        try:
            # Adjust path if necessary depending on execution context
            # This assumes the script is run from the project root or src
            # A better approach for production is to load from a known config path
            # or package data. For now, relative path from this file:
            current_dir = os.path.dirname(__file__)
            # Go up two directories (from services to src) and then into prompts
            template_path = os.path.join(current_dir, "..", "prompts", "chitchat_response.txt")

            # Fallback/alternative path if running from project root
            if not os.path.exists(template_path):
                 template_path = os.path.join("src", "prompts", "chitchat_response.txt")


            with open(template_path, "r", encoding="utf-8") as f:
                template = f.read().strip()
            logger.info(f"Loaded chit-chat prompt template from {template_path}")
            return template
        except FileNotFoundError:
            logger.error(f"Chit-chat prompt template file not found at {template_path}")
            # Provide a basic fallback template or raise an error
            fallback_template = "You are a friendly assistant. Respond conversationally to the user's input: {user_query}"
            logger.warning("Using fallback chit-chat prompt template.")
            return fallback_template
        except Exception as e:
            logger.exception(f"Error loading chit-chat prompt template: {e}")
            fallback_template = "You are a friendly assistant. Respond conversationally to the user's input: {user_query}"
            logger.warning("Using fallback chit-chat prompt template due to error.")
            return fallback_template


    def generate_response(self, user_query: str, **llm_kwargs: Any) -> str:
        """
        Generates a conversational response for a chit-chat query using the LLM.

        Args:
            user_query: The user's input query classified as chit-chat.
            **llm_kwargs: Additional keyword arguments to pass to the LLM call
                          (e.g., temperature, max_tokens).

        Returns:
            A natural language response generated by the LLM.
        """
        if not self._prompt_template:
             logger.error("Chit-chat prompt template is not loaded. Cannot generate response.")
             return "I'm sorry, I can't generate a response right now."

        try:
            # Format the prompt with the user's query
            prompt = self._prompt_template.format(user_query=user_query)
            logger.debug(f"Formatted chit-chat prompt: {prompt[:200]}...")

            # Use the LLMInteractionService to get the completion
            # Pass through any additional LLM kwargs
            response = self.llm_service.get_completion(prompt=prompt, **llm_kwargs)

            logger.info("Successfully generated chit-chat response.")
            return response

        except Exception as e:
            logger.exception(f"Error generating chit-chat response for query '{user_query[:50]}...': {e}")
            # Depending on requirements, you might return a generic error message
            # or re-raise the exception. Returning a message for user-friendliness.
            return "I'm having trouble responding right now. Please try again later."

# Example usage (for testing instantiation)
if __name__ == "__main__":
    # This block requires a running LLMInteractionService or a mock
    logging.basicConfig(level=logging.DEBUG)
    logger.info("Testing ChitChatHandlerModule instantiation.")

    # Assume settings are loaded (as done in core/config __main__)
    try:
        from core.config import settings
        print("Using settings from core.config.")
        # Ensure OPENAI_API_KEY is set in your environment or .env file
        if not settings.OPENAI_API_KEY or settings.OPENAI_API_KEY == "sk-mock-key-1234":
             print("WARNING: OPENAI_API_KEY is not set or is a mock key. Actual LLM calls will fail.")
             # Mock the LLMInteractionService for testing without a real key
             import unittest.mock
             class MockLLMService:
                 def get_completion(self, prompt: str, **kwargs: Any) -> str:
                     print(f"Mock LLM received prompt: {prompt[:100]}...")
                     # Simulate a delay to test performance criteria mentally
                     import time
                     time.sleep(0.1) # Simulate quick response
                     if "{user_query}" in prompt:
                         # Extract query from formatted prompt (basic attempt)
                         parts = prompt.split(":")
                         mock_query = parts[-1].strip() if len(parts) > 1 else "user input"
                         return f"Mock response to: '{mock_query}'. I am a friendly mock assistant!"
                     return "Mock LLM response."

             llm_service_instance = MockLLMService()
             print("Using MockLLMService.")
        else:
             llm_service_instance = LLMInteractionService(settings)
             print("Using actual LLMInteractionService.")


        # Instantiate the handler
        chitchat_handler = ChitChatHandlerModule(llm_service=llm_service_instance)
        print("ChitChatHandlerModule instantiated successfully.")

        # Test generating a response
        test_query = "Hello, how are you?"
        print(f"\nGenerating response for query: '{test_query}'")
        start_time = time.time()
        response = chitchat_handler.generate_response(test_query)
        end_time = time.time()
        print(f"Generated response: '{response}'")
        print(f"Response time: {end_time - start_time:.2f} seconds")

        test_query_2 = "Tell me a joke."
        print(f"\nGenerating response for query: '{test_query_2}'")
        start_time = time.time()
        response_2 = chitchat_handler.generate_response(test_query_2)
        end_time = time.time()
        print(f"Generated response: '{response_2}'")
        print(f"Response time: {end_time - start_time:.2f} seconds")


    except ImportError:
        print("Could not import settings from core.config. Ensure core.config is runnable or mock dependencies.")
    except Exception as e:
        logger.exception(f"An error occurred during testing: {e}")